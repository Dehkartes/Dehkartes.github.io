---
title: "Attention의 Query, Key, Value"
categories: "AI"
tags: ["#AI", "#NLP"]
toc: true
toc_sticky: true
date: 2024-12-05 00:00:00 +0900
last_modified_at: 2024-12-05 00:00:00 +0900
---
# 1. Query, Key, Value

Transformer의 Attention은 입력을 Q(Query), K(Key), V(Value)로 구분하는데, Q는 비교할 주체가 되는 벡터, K는 비교할 객체가 되는 벡터, V는 또다른 비교할 객체가 되는 벡터다. 이때 Q와 K가 유사할수록 내적한 값이 커지는데, 이를 V와 곱하면 Q와 K의 유사도만큼 강화된 V가 된다.

아래 Seq2Seq 모델 디코더의 두 번째 문맥 벡터 $C_2$를 구하는 식에서

$C_2=<\hat h_2,h_1>h_1+<\hat h_2,h_2>h_2+<\hat h_2,h_3>h_3$

$\hat h_2=Q$, <>안 $h_i=K$,  <>밖 $h_i=V$가 된다
Transformer에서는 K와 V가 서로 다른 선형 변환을 거치기 때문에 서로 다른 벡터가 된다.