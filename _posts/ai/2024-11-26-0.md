---
title: "Sequence to Sequence 이해"
categories: "AI"
tags: ["#AI", "#NLP"]
toc: true
toc_sticky: true
date: 2024-11-26 00:00:00 +0900
last_modified_at: 2024-11-26 00:00:00 +0900
---
# 1. Sequence to Sequence

기존 RNN 모델은 정적 매핑(선형 회귀, 분류)을 벗어나는 문제를 해결할 수 없다. 시퀀스(입력 문자열)의 맥락, 구조 의존성을 고려하고, 아래 예시 처럼 입력과 출력의 형태가 다른 문제를 해결하기위해 Seq2Seq(Sequence to Sequence)구조가 사용된다.

- 번역: Hello, world! → 세상아, 안녕!(같은 의미를 가진 단어의 순서가 다름)
- 긴 문장 → 짤은문장 요약

![image.png]({{ site.url }}{{ site.baseurl }}\assets\post\ai\2024-11-26-0\image.png)

흔히 Seq2Seq 를 설명할 때 볼 수 있는 그림이다. 이를 이해해보자.

## 1.1 Encoder

- 매 시점(단계)마다 현 시점의 임베딩 벡터와 이전 시점의 은닉 상태(Hidden State: 현재까지의 문맥을 압축한 벡터)를 인코더 모델에 입력하고 새로운 은닉 상태를 출력
- 위 과정을 반복하여 문백 벡터(최종 은닉 상태)를 디코더에 전달
- $x_t$
	- $t$ 시점에 인코더에 입력되는 토큰의 임베딩 벡터
- $h_t$
	- 인코더 *t* 시점의 은닉 상태: 인코더에 $h_{t-1}$과 $x_1$을 입력했을 때 출력되는 벡터이며, $h_0\,...\,h_{t-1}$가 누적된 누적벡터
	- $h_0$: 입력 시퀀스를 처리하기 전에 설정되는 초기 은닉 상태, 영백터나 훈련 가능한 파라미터로 초기화
	- $h_T$: 인코더의 최종 은닉 상태, 즉 문맥벡터

## 1.2 Decoder

- 처음에는 인코더로부터 전달받은 문맥 벡터로 은닉상태를 초기화하고 임베딩된 시작 토큰과 함께 디코더 모델에 입력, 다음부터는 이전 시점의 토큰을 임베딩하여 이전 시점의 은닉 상태와 입력, 로짓 출력
- 종료 토큰이 출력될 때 까지의 모든 디코더 출력이 최종 결과
- $y_t$
	- $\hat{h}_{t-1}$와 $y_{t-1}$를 입력한 디코더의 출력
	- t 시점의 디코더 은닉상태를 선형 변환(로짓 계산)한 벡터
	- $y_t$의 확률 분포를 구하여(Softmax 사용) 가장 높은 확률을 가진 토큰 선택
- $*\hat{h}_t*$
	- 디코더 *t* 시점의 은닉상태: 디코더에 $\hat{h}_{t-1}$과 $y_i$를 입력했을 때 생성되는 벡터이며, $h_0\,...\,h_{T},\,\hat{h}_1\,...\,\hat{h}_{t-1}$가 누적된 누적벡터
	- $\hat{h}_0 = h_T$

# 2. 예시

영어를 한글로 번역하는 Seq2Seq모델로 “Hello, world!”를 “세계야, 안녕!”으로 변환

1. 원문 토크나이징
	- 토크나이저로 정수(토큰 ID) 매핑
	- “Hello, world!” → [”Hello”, “,”, “world”, “!”] → [12, 15, 22, 50] **예시를 위한 임의 수치*
2. 임베딩
	- 각 입력 토큰을 임베딩 레이어를 통해 고정 길이 벡터로 변환
	- [12, 15, 22, 50]→ [[0.225, 0.241, …], [0.345, -0.211, …], [0.563, 0.248, …], [-0.369, 0.488, …] ] **예시를 위한 임의 수치*
3. 인코딩
	1. 입력 임베딩을 순차적으로 처리, 각 시점의 은닉 상태 계산
	2. 최종 문맥 벡터 생성
4. 디코딩
	1. 은닉 상태를 인코더의 최종 문맥 벡터로 초기화
	2. 첫 “세계야” 토큰 생성
		1. 시작 토큰의 임베딩 벡터와 a 단계의 은닉 상태를 디코더에 입력
		2. 은닉 상태 갱신
		3. 출력 로짓 계산(가중치X은닉 상태+편향 = 로짓)
		4. 확률 분포 계산(Softmax(출력 로짓))
		5. 가장 확률이 높은 단어 토큰 선택(”세계야”의 토큰ID)
		6. 토큰 ID를 문자열로 변환(”세계야”)
	3. “,”, “안녕”, “!”, 종료 토큰 생성
		1. 이전 시점의 토큰ID를 임베딩 벡터와 이전 시점의 은닉 상태를 디코더에 입력
		2. 은닉 상태 갱신
		3. 출력로짓계산→확률분포계산→토큰 선택→문자열 변환
		4. 종료 토큰 생성까지 반복
