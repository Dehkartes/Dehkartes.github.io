---
title: "인공 신경망(Neural network) 이해"
categories: "AI"
tags: ["#AI", "#NLP"]
toc: true
toc_sticky: true
date: 2025-01-01 00:00:00 +0900
last_modified_at: 2025-01-04 23:22:49 +0900
---
# 1. 퍼셉트론(Perceptron)

![퍼셉트론.png]({{ site.url }}{{ site.baseurl }}\assets\post\ai\2025-01-01-4\퍼셉트론.png)

> $x_i$: 입력값, $w_i$: 가중치, $b$: 편향,  $\sigma$: 활성화 함수, $y$: 출력값
> 

퍼셉트론은 초기 형태의 인공 신경망으로 다수의 입력을 하나의 출력으로 내보내는 알고리즘이다.

가중치와 편향은 파라미터로 훈련을 통해 값이 정해지며 추론 과정은 다음과 같다.

1. 입력값과 가중치의 가중합($\sum\limits_{i=1}^n x_iw_i = x_1w_1+x_2w_2+...+x_nw_n$) 연산
2. 가중합과 편향 합연산
3. 결과 2에 활성화 함수를 취하여(신경망에 비선형성 부여) 최종 결과 도출

# 2. 다층 퍼셉트론(Multilayer Perceptron, MLP)

MLP는 다수의 층을 가지고있고 다수의 퍼셉트론이 층을 구성한다. 이때 MLP에선 퍼셉트론을 노드라 한다.

![인공신경망.svg]({{ site.url }}{{ site.baseurl }}\assets\post\ai\2025-01-01-4\인공신경망.svg)

은닉층 1의 맨 위 노드를 보자.
3개의 모든 입력과 연결되어있고, 각 입력이 퍼셉트론의 $x_i$다. 마찬가지로 연결된 선이 $w_i$가 되고 각 노드는 최종 결과 $y$를 가질것이다.

은닉층 2에서는 각 노드가 은닉층 1의 결과를 $x_i$로 가지며 동일한 구조가 출력층 까지 반복된다.

전체 구조를 보면 한 층의 모든 노드가 다음 층의 모든 노드와 연결된 것을 알 수 있는데, 이를 완전 연결 계층(Fully connected layer)이라 한다.

위 그림을 코드로 구현하면 아래처럼 된다.

```python
class NeuralNetwork(nn.Module):
	def **init**(self):
	super(SimpleNN, self).**init**()
		# 입력층(3) -> 은닉층1(4)
		self.hidden1 = nn.Linear(3, 4)
		# 은닉층1(4) -> 은닉층2(4)
		self.hidden2 = nn.Linear(4, 4)
		# 은닉층2(4) -> 은닉층3(4)
		self.hidden3 = nn.Linear(4, 4)
		# 은닉층3(4) -> 출력층(2)
		self.output = nn.Linear(4, 2)

	def forward(self, x):
		# 은닉층1 연산 및 활성화 함수 (ReLU)
		x = F.relu(self.hidden1(x))

		# 은닉층2 연산 및 활성화 함수 (ReLU)
		x = F.relu(self.hidden2(x))

		# 은닉층3 연산 및 활성화 함수 (ReLU)
		x = F.relu(self.hidden3(x))

		# 출력층 연산 (활성화 함수 없음)
		x = self.output(x)
		return x
```