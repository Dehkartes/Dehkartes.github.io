---
title: "Attention 이해"
categories: "AI"
tags: ["#AI", "#NLP"]
toc: true
toc_sticky: true
date: 2024-12-01 00:00:00 +0900
last_modified_at: 2024-12-01 00:00:00 +0900
---
# 1. Attention

Sequence to Sequence에서 RNN기반 모델의 특징에의해 벡터가 누적 될 때 마다 오래된 정보는 약화 되고 마지막 토큰에 가중치가 실린다.

예를 들면, Hello, world! → 세계야, 안녕! 을 번역할 때, “세계야” 토큰과 가장 연관있는 토큰은 “world” 지만 인코더의 마지막 입력 토큰으로 들어간 “!” 에 가중치가 실린다.

이에 디코더에서 문맥 벡터를 만들 때 인코더의 모든 시점의 출력과 연산하여 입력 토큰의 연관성에 비례해 가중치를 두는 알고리즘이 Attention이다.

![image.png]({{ site.url }}{{ site.baseurl }}\assets\post\ai\2024-12-01-2\image.png)

Seq2Seq 모델에서 디코딩 과정에 Attention을 사용하게 된다.

1. 인코더에서 각 시점의 $h_t$생성
2. 인코더에서 최종 은닉상태 $h_T$생성
3. 디코더에서 은닉 상태 계산
    1. $\hat{h}\_{t-1}$(이전 시점 은닉 상태)과 $y\_{t-1}$(이전 시점 디코더 출력이나 시작 토큰)의 임베딩 토큰으로 문맥 벡터 $c\_t$ 생성
    2. $c_t$와 인코더의 모든 $h_i$을 내적하여 Attention Score계산 (Attention 시작)
        
        > 두 벡터를 내적하면 스칼라값이 나오며, 이는 벡터 간의 유사성을 나타내며,
        내적한 값이 클 수록 두 벡터는 상관관계가 크다고 볼 수 있다.
        즉 $c\_t$과 각 $h\_i$를 내적한 값들은 각 $h\_i$가  $c\_t$와 얼마나 유사한지 나타낸다.
        > 
    3. Attention Score를 Softmax로 정규화하여 Attention Weight 계산
    4. Attention Weight와 모든 $h\_i$의 가중합 연산으로 문맥 벡터$c'\_t$ 생성
        
        > $c'\_t$ 는 인코더의 각 토큰의 중요함이 포함된 문맥 벡터
        > 
    5. 디코더에 $c_t$와 $c'_t$ 를 입력하여 $\hat{h}_t$ 출력 (Attention 끝)